# è®­ç»ƒå¡ä½é—®é¢˜è¯Šæ–­å’Œè§£å†³æ–¹æ¡ˆ

## ğŸ” é—®é¢˜è¯Šæ–­

ä»è¿›ç¨‹åˆ—è¡¨å¯ä»¥çœ‹åˆ°ï¼š
- æœ‰å¤§é‡è®­ç»ƒè¿›ç¨‹åœ¨è¿è¡Œï¼ˆå¯èƒ½æœ‰é‡å¤å¯åŠ¨ï¼‰
- éƒ¨åˆ†è¿›ç¨‹CPUä½¿ç”¨ç‡99.6%ï¼ˆæ­£å¸¸è¿è¡Œï¼‰
- éƒ¨åˆ†è¿›ç¨‹CPUä½¿ç”¨ç‡0.0%ï¼ˆå¯èƒ½å¡ä½ï¼‰

è®­ç»ƒå¡åœ¨Epoch 223çš„98%è¿›åº¦ï¼Œå¯èƒ½çš„åŸå› ï¼š

### 1. æ•°æ®åŠ è½½å™¨æ­»é”ï¼ˆæœ€å¯èƒ½ï¼‰
- DDPæ¨¡å¼ä¸‹ï¼Œ`num_workers=4` å¯èƒ½å¯¼è‡´æ­»é”
- ç‰¹åˆ«æ˜¯åœ¨éªŒè¯é˜¶æ®µæˆ–epochç»“æŸæ—¶

### 2. DDP BarrieråŒæ­¥é—®é¢˜
- æŸä¸ªGPUè¿›ç¨‹æ²¡æœ‰åˆ°è¾¾barrierï¼Œå¯¼è‡´å…¶ä»–è¿›ç¨‹ç­‰å¾…
- å¸¸è§äºéªŒè¯é˜¶æ®µæˆ–æ¨¡å‹ä¿å­˜æ—¶

### 3. æ–‡ä»¶I/Oé˜»å¡
- å¯è§†åŒ–ä¿å­˜æ—¶å¯èƒ½é˜»å¡
- Checkpointä¿å­˜æ—¶å¯èƒ½é˜»å¡

## ğŸš¨ ç«‹å³è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ1ï¼šæ€æ­»å¡ä½çš„è¿›ç¨‹å¹¶é‡å¯ï¼ˆæ¨èï¼‰

```bash
# 1. æ€æ­»æ‰€æœ‰è®­ç»ƒè¿›ç¨‹
pkill -f "train.py"

# 2. ç­‰å¾…å‡ ç§’ç¡®ä¿è¿›ç¨‹å®Œå…¨é€€å‡º
sleep 5

# 3. æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ®‹ç•™è¿›ç¨‹
ps aux | grep train.py | grep -v grep

# 4. ä¿®æ”¹è®­ç»ƒå‚æ•°ï¼Œå‡å°‘num_workers
# ç¼–è¾‘ train.pyï¼Œå°† num_workers=4 æ”¹ä¸º num_workers=0 æˆ– 2
```

### æ–¹æ¡ˆ2ï¼šä¿®æ”¹ä»£ç é¿å…æ­»é”

ä¿®æ”¹ `train.py` ä¸­çš„æ•°æ®åŠ è½½å™¨é…ç½®ï¼š

```python
# åœ¨ train.py ä¸­ï¼Œæ‰¾åˆ° DataLoader åˆ›å»ºçš„åœ°æ–¹ï¼ˆçº¦295è¡Œå’Œ311è¡Œï¼‰
# å°† num_workers=4 æ”¹ä¸º num_workers=0 æˆ– 2
train_loader = DataLoader(
    train_dataset,
    batch_size=args.batch_size,
    sampler=train_sampler,
    collate_fn=collate_fn,
    num_workers=0,  # æ”¹ä¸º0é¿å…æ­»é”
    pin_memory=True
)
```

### æ–¹æ¡ˆ3ï¼šä½¿ç”¨æ›´å®‰å…¨çš„DDPé…ç½®

æ·»åŠ è¶…æ—¶å’Œé”™è¯¯å¤„ç†ï¼š

```bash
# è®¾ç½®æ›´é•¿çš„NCCLè¶…æ—¶
export NCCL_TIMEOUT=3600
export NCCL_DEBUG=INFO

# é‡æ–°å¯åŠ¨è®­ç»ƒ
torchrun --nproc_per_node=8 train.py ...
```

## ğŸ”§ ä¿®å¤ä»£ç 

### ä¿®å¤1ï¼šå‡å°‘num_workersé¿å…æ­»é”

ä¿®æ”¹ `train.py` ä¸­çš„æ•°æ®åŠ è½½å™¨ï¼š

```python
# çº¦295è¡Œ
train_loader = DataLoader(
    train_dataset,
    batch_size=args.batch_size,
    sampler=train_sampler,
    collate_fn=collate_fn,
    num_workers=0,  # æ”¹ä¸º0ï¼Œé¿å…DDPæ­»é”
    pin_memory=True,
    persistent_workers=False  # æ·»åŠ è¿™ä¸ª
)

# çº¦311è¡Œ
val_loader = DataLoader(
    val_dataset,
    batch_size=args.batch_size,
    sampler=val_sampler,
    collate_fn=collate_fn,
    num_workers=0,  # æ”¹ä¸º0
    pin_memory=True,
    persistent_workers=False  # æ·»åŠ è¿™ä¸ª
)
```

### ä¿®å¤2ï¼šæ·»åŠ è¶…æ—¶ä¿æŠ¤

åœ¨ `trainer.py` çš„éªŒè¯å’Œä¿å­˜æ“ä½œä¸­æ·»åŠ è¶…æ—¶ï¼š

```python
# åœ¨éªŒè¯åæ·»åŠ è¶…æ—¶ä¿æŠ¤
if self.ddp_enabled:
    import torch.distributed as dist
    try:
        dist.barrier(timeout=timedelta(seconds=300))  # 5åˆ†é’Ÿè¶…æ—¶
    except Exception as e:
        if self.rank == 0:
            print(f"âš ï¸  Barrierè¶…æ—¶: {e}")
```

## ğŸ“‹ é‡å¯è®­ç»ƒæ­¥éª¤

### æ­¥éª¤1ï¼šæ¸…ç†è¿›ç¨‹

```bash
# æ€æ­»æ‰€æœ‰è®­ç»ƒè¿›ç¨‹
pkill -f "train.py"
pkill -f "torchrun"

# ç¡®è®¤è¿›ç¨‹å·²é€€å‡º
ps aux | grep -E "(train|torchrun)" | grep -v grep
```

### æ­¥éª¤2ï¼šä¿®æ”¹ä»£ç ï¼ˆå¦‚æœéœ€è¦ï¼‰

å¦‚æœä¹‹å‰æ²¡æœ‰ä¿®æ”¹ï¼Œç°åœ¨ä¿®æ”¹ `train.py`ï¼š

```bash
cd /home/Oliver-0402/--/protein_mdm
# å°† num_workers=4 æ”¹ä¸º num_workers=0
```

### æ­¥éª¤3ï¼šé‡æ–°å¯åŠ¨è®­ç»ƒ

```bash
cd /home/Oliver-0402/--/protein_mdm

# è®¾ç½®ç¯å¢ƒå˜é‡
export NCCL_TIMEOUT=3600

# é‡æ–°å¯åŠ¨ï¼ˆä½¿ç”¨ä¿®å¤åçš„å‚æ•°ï¼‰
torchrun --nproc_per_node=8 train.py \
    --pdb_path data/cache \
    --cache_dir data/cache \
    --use_predefined_split \
    --resume checkpoints/checkpoint_epoch_220.pt \
    --epochs 300 \
    --batch_size 4 \
    --learning_rate 2e-4 \
    --weight_decay 1e-4 \
    --warmup_epochs 20 \
    --early_stopping_patience 30 \
    --early_stopping_min_delta 0.001 \
    --use_discrete_diffusion \
    --num_diffusion_steps 1000 \
    --masking_strategy random \
    --save_dir checkpoints \
    --visualize \
    --plot_every 5
```

## âš ï¸ é¢„é˜²æªæ–½

1. **å‡å°‘num_workers**: åœ¨DDPæ¨¡å¼ä¸‹ï¼Œä½¿ç”¨ `num_workers=0` æˆ– `2`ï¼Œé¿å…æ­»é”
2. **æ·»åŠ è¶…æ—¶**: åœ¨barrieræ“ä½œä¸­æ·»åŠ è¶…æ—¶ä¿æŠ¤
3. **ç›‘æ§è¿›ç¨‹**: å®šæœŸæ£€æŸ¥è¿›ç¨‹çŠ¶æ€
4. **ä¿å­˜checkpoint**: ç¡®ä¿å®šæœŸä¿å­˜ï¼Œé¿å…ä¸¢å¤±è¿›åº¦

## ğŸ” è¿›ä¸€æ­¥è¯Šæ–­

å¦‚æœé—®é¢˜æŒç»­ï¼Œå¯ä»¥ï¼š

1. **æŸ¥çœ‹æ—¥å¿—**:
   ```bash
   # æŸ¥çœ‹æœ€è¿‘çš„è®­ç»ƒæ—¥å¿—
   tail -f nohup.out  # å¦‚æœæœ‰nohupè¿è¡Œ
   ```

2. **æ£€æŸ¥GPUçŠ¶æ€**:
   ```bash
   nvidia-smi
   ```

3. **æ£€æŸ¥NCCLé€šä¿¡**:
   ```bash
   export NCCL_DEBUG=INFO
   # é‡æ–°è¿è¡Œè®­ç»ƒï¼ŒæŸ¥çœ‹è¯¦ç»†æ—¥å¿—
   ```

4. **ä½¿ç”¨å•GPUæµ‹è¯•**:
   ```bash
   # å…ˆç”¨å•GPUæµ‹è¯•æ˜¯å¦æ­£å¸¸
   python train.py --pdb_path data/cache --cache_dir data/cache ...
   ```

## ğŸ“ æ€»ç»“

**æœ€å¯èƒ½çš„åŸå› **: DataLoaderçš„ `num_workers=4` åœ¨DDPæ¨¡å¼ä¸‹å¯¼è‡´æ­»é”

**æœ€å¿«è§£å†³æ–¹æ¡ˆ**: 
1. æ€æ­»å¡ä½çš„è¿›ç¨‹
2. ä¿®æ”¹ `num_workers=0`
3. é‡æ–°å¯åŠ¨è®­ç»ƒ

**é•¿æœŸè§£å†³æ–¹æ¡ˆ**: æ·»åŠ è¶…æ—¶ä¿æŠ¤å’Œæ›´å¥½çš„é”™è¯¯å¤„ç†
