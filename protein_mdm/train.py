"""
è®­ç»ƒè„šæœ¬

ä½¿ç”¨æ–¹æ³•:
    python train.py --pdb_path data/pdb_files --epochs 50 --batch_size 4
    æˆ–è€…:
    python -m train --pdb_path data/pdb_files --epochs 50 --batch_size 4
"""

import argparse
import sys
import os
from pathlib import Path
from datetime import timedelta

import matplotlib
# å¼ºåˆ¶ä½¿ç”¨æ— çª—å£åç«¯
matplotlib.use('Agg') 
import matplotlib.pyplot as plt

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„ï¼Œä»¥æ”¯æŒç›¸å¯¹å¯¼å…¥
project_root = Path(__file__).parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# æ³¨æ„ï¼štorch çš„å¯¼å…¥å°†åœ¨ if __name__ == "__main__" ä¸­ï¼Œåœ¨è®¾ç½® CUDA_VISIBLE_DEVICES ä¹‹å


def load_train_val_splits(cache_dir: str):
    """
    ä»ç¼“å­˜ç›®å½•åŠ è½½é¢„å®šä¹‰çš„è®­ç»ƒé›†å’ŒéªŒè¯é›†
    
    Args:
        cache_dir: ç¼“å­˜ç›®å½•è·¯å¾„
        
    Returns:
        (train_paths, val_paths) å…ƒç»„ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™è¿”å› (None, None)
    """
    train_file = os.path.join(cache_dir, 'train.txt')
    val_file = os.path.join(cache_dir, 'val.txt')
    
    train_paths = None
    val_paths = None
    
    # åŠ è½½è®­ç»ƒé›†
    if os.path.exists(train_file):
        with open(train_file, 'r') as f:
            train_files = [line.strip() for line in f if line.strip()]
        # æ„å»ºå®Œæ•´çš„ç¼“å­˜æ–‡ä»¶è·¯å¾„
        train_paths = [
            os.path.join(cache_dir, f"{name}.pt")
            for name in train_files
            if os.path.exists(os.path.join(cache_dir, f"{name}.pt"))
        ]
    
    # åŠ è½½éªŒè¯é›†
    if os.path.exists(val_file):
        with open(val_file, 'r') as f:
            val_files = [line.strip() for line in f if line.strip()]
        # æ„å»ºå®Œæ•´çš„ç¼“å­˜æ–‡ä»¶è·¯å¾„
        val_paths = [
            os.path.join(cache_dir, f"{name}.pt")
            for name in val_files
            if os.path.exists(os.path.join(cache_dir, f"{name}.pt"))
        ]
    
    return train_paths, val_paths


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="è®­ç»ƒè›‹ç™½è´¨ä¾§é“¾è®¾è®¡æ¨¡å‹"
    )
    
    # æ•°æ®å‚æ•°
    parser.add_argument("--pdb_path", type=str, required=True,
                       help="PDB æ–‡ä»¶è·¯å¾„æˆ–ç›®å½•ï¼ˆæˆ–ç¼“å­˜ç›®å½•ï¼‰")
    parser.add_argument("--cache_dir", type=str, default=None,
                       help="ç¼“å­˜ç›®å½•ï¼ˆå¦‚æœæä¾›ï¼Œå°†ä½¿ç”¨ç¼“å­˜åŠ é€ŸåŠ è½½ï¼‰")
    parser.add_argument("--batch_size", type=int, default=4,
                       help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--val_split", type=float, default=0.1,
                       help="éªŒè¯é›†æ¯”ä¾‹ï¼ˆå¦‚æœä½¿ç”¨é¢„å®šä¹‰åˆ’åˆ†ï¼Œåˆ™å¿½ç•¥æ­¤å‚æ•°ï¼‰")
    parser.add_argument("--use_predefined_split", action="store_true",
                       help="ä½¿ç”¨é¢„å®šä¹‰çš„ train.txt å’Œ val.txt åˆ’åˆ†ï¼ˆå¦‚æœå­˜åœ¨ï¼‰")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--hidden_dim", type=int, default=256,
                       help="éšè—å±‚ç»´åº¦")
    parser.add_argument("--num_encoder_layers", type=int, default=3,
                       help="Encoder å±‚æ•°")
    parser.add_argument("--num_decoder_layers", type=int, default=3,
                       help="Decoder å±‚æ•°")
    parser.add_argument("--num_heads", type=int, default=8,
                       help="æ³¨æ„åŠ›å¤´æ•°")
    parser.add_argument("--dropout", type=float, default=0.3,
                       help="Dropout ç‡ï¼ˆé»˜è®¤ 0.3ï¼Œç”¨äºå¢å¼ºæ­£åˆ™åŒ–ï¼‰")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--epochs", type=int, default=300,
                       help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--learning_rate", type=float, default=5e-4,
                       help="å­¦ä¹ ç‡ï¼ˆæœ€å¤§å­¦ä¹ ç‡ï¼‰")
    parser.add_argument("--warmup_epochs", type=int, default=20,
                       help="Warmup è½®æ•°")
    parser.add_argument("--weight_decay", type=float, default=1e-3,
                       help="æƒé‡è¡°å‡ï¼ˆé»˜è®¤ 1e-3ï¼Œç”¨äºå¢å¼ºæ­£åˆ™åŒ–ï¼‰")
    parser.add_argument("--masking_strategy", type=str, default="random",
                       choices=["random", "block"],
                       help="æ©ç ç­–ç•¥")
    parser.add_argument("--num_diffusion_steps", type=int, default=1000,
                       help="æ‰©æ•£æ¨¡å‹çš„æ—¶é—´æ­¥æ•°ï¼ˆé»˜è®¤1000ï¼‰")
    parser.add_argument("--label_smoothing", type=float, default=0.1,
                       help="äº¤å‰ç†µæ ‡ç­¾å¹³æ»‘ç³»æ•°ï¼ˆé»˜è®¤0.1ï¼Œè¯Šæ–­å¯å°è¯•0.0/0.05ï¼‰")
    parser.add_argument("--max_train_samples", type=int, default=0,
                       help="ä»…ä½¿ç”¨å‰Nä¸ªè®­ç»ƒæ ·æœ¬ï¼ˆ0è¡¨ç¤ºä¸é™åˆ¶ï¼Œç”¨äºå¿«é€Ÿè¯Šæ–­ï¼‰")
    parser.add_argument("--max_val_samples", type=int, default=0,
                       help="ä»…ä½¿ç”¨å‰Nä¸ªéªŒè¯æ ·æœ¬ï¼ˆ0è¡¨ç¤ºä¸é™åˆ¶ï¼Œç”¨äºå¿«é€Ÿè¯Šæ–­ï¼‰")
    parser.add_argument("--overfit_train_subset", type=int, default=0,
                       help="å®¹é‡è¯Šæ–­ï¼šä»è®­ç»ƒé›†å–Nä¸ªæ ·æœ¬ï¼Œå¹¶å°†éªŒè¯é›†ä¹Ÿè®¾ä¸ºåŒä¸€å­é›†ï¼ˆ0è¡¨ç¤ºå…³é—­ï¼‰")
    
    # å…¶ä»–å‚æ•°
    parser.add_argument("--save_dir", type=str, default="checkpoints",
                       help="ä¿å­˜ç›®å½•")
    parser.add_argument("--device", type=str, default=None,
                       help="è®¾å¤‡ (cuda/cpu)")
    parser.add_argument("--visualize", action="store_true", default=True,
                       help="å¯ç”¨è®­ç»ƒå¯è§†åŒ–ï¼ˆé»˜è®¤å¯ç”¨ï¼‰")
    parser.add_argument("--no_visualize", dest="visualize", action="store_false",
                       help="ç¦ç”¨è®­ç»ƒå¯è§†åŒ–")
    parser.add_argument("--plot_every", type=int, default=5,
                       help="æ¯ N ä¸ª epoch ç»˜åˆ¶ä¸€æ¬¡å›¾è¡¨ï¼ˆé»˜è®¤ 5ï¼‰")
    parser.add_argument("--resume", type=str, default=None,
                       help="ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒï¼ˆæä¾›checkpointè·¯å¾„ï¼Œå¦‚ checkpoints/best_model.ptï¼‰")
    parser.add_argument("--early_stopping_patience", type=int, default=50,
                       help="æ—©åœè€å¿ƒå€¼ï¼ŒéªŒè¯æŸå¤±è¿ç»­Nè½®ä¸ä¸‹é™åˆ™åœæ­¢è®­ç»ƒï¼ˆé»˜è®¤ 50ï¼Œç»™æ¨¡å‹æ›´å¤šéœ‡è¡æ”¶æ•›çš„æ—¶é—´ï¼‰")
    parser.add_argument("--early_stopping_min_delta", type=float, default=0.0,
                       help="æ—©åœæœ€å°æ”¹è¿›é˜ˆå€¼ï¼Œåªæœ‰æ”¹è¿›è¶…è¿‡æ­¤å€¼æ‰è®¤ä¸ºæ˜¯æœ‰æ•ˆæ”¹è¿›")
    parser.add_argument("--debug_mode", action="store_true",
                       help="å¯ç”¨è¯¦ç»†è°ƒè¯•æ—¥å¿—ï¼ˆåŒ…å«tracebackå’Œå¼ é‡ç»Ÿè®¡ä¿¡æ¯ï¼‰")
    
    # DDP ç›¸å…³å‚æ•°
    parser.add_argument("--gpu_ids", type=str, default=None,
                       help="æŒ‡å®šä½¿ç”¨çš„GPU IDï¼ˆä¾‹å¦‚ï¼š'1,2,3,4,5,6,7'ï¼‰ï¼Œåœ¨æ‰€æœ‰torchè°ƒç”¨ä¹‹å‰è®¾ç½®CUDA_VISIBLE_DEVICES")
    parser.add_argument("--master_port", type=str, default="29500",
                       help="DDP master portï¼ˆé»˜è®¤ï¼š29500ï¼‰")
    parser.add_argument("--ddp", action="store_true",
                       help="å¯ç”¨ DDP æ¨¡å¼ï¼ˆé€šå¸¸ç”± torchrun è‡ªåŠ¨æ£€æµ‹ï¼Œæ­¤å‚æ•°ç”¨äºæ‰‹åŠ¨æ¨¡å¼ï¼‰")
    
    return parser.parse_args()


def main():
    # ç¼“è§£æ˜¾å­˜ç¢ç‰‡åŒ–ï¼Œé™ä½ OOM æ¦‚ç‡ï¼ˆå¿…é¡»åœ¨ä»»ä½• torch/cuda è°ƒç”¨ä¹‹å‰è®¾ç½®ï¼‰
    os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
    # å…³é”®ï¼šåœ¨ main å‡½æ•°æœ€å¼€å§‹è§£æå‚æ•°å¹¶è®¾ç½® CUDA_VISIBLE_DEVICES
    # è¿™å¿…é¡»åœ¨ä»»ä½• torch cuda è°ƒç”¨ä¹‹å‰å®Œæˆ
    args = globals().get('_args')
    if args is None:
        args = parse_args()
    
    # åœ¨æ‰€æœ‰ torch cuda è°ƒç”¨ä¹‹å‰è®¾ç½® CUDA_VISIBLE_DEVICES
    if args.gpu_ids:
        os.environ["CUDA_VISIBLE_DEVICES"] = args.gpu_ids
        # åªåœ¨é DDP æ¨¡å¼ä¸‹æ‰“å°ï¼Œé¿å…å¤šè¿›ç¨‹é‡å¤æ‰“å°
        if "RANK" not in os.environ and "LOCAL_RANK" not in os.environ:
            print(f"âœ… å·²è®¾ç½® CUDA_VISIBLE_DEVICES={args.gpu_ids}")
    
    # å¯¼å…¥å¿…è¦çš„æ¨¡å—ï¼ˆåœ¨è®¾ç½® CUDA_VISIBLE_DEVICES ä¹‹åï¼‰
    import torch
    import torch.distributed as dist
    from torch.utils.data import DataLoader
    from torch.utils.data.distributed import DistributedSampler
    
    from models.encoder import BackboneEncoder
    from models.decoder import FragmentDecoder
    from data.dataset import ProteinStructureDataset, collate_fn
    from data.vocabulary import get_vocab
    from training.trainer import Trainer
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ DDPï¼ˆé€šè¿‡ç¯å¢ƒå˜é‡åˆ¤æ–­ï¼Œtorchrun ä¼šè‡ªåŠ¨è®¾ç½®ï¼Œæˆ–é€šè¿‡ --ddp å‚æ•°ï¼‰
    ddp_enabled = args.ddp or (
        "RANK" in os.environ and "WORLD_SIZE" in os.environ
    ) or (
        "LOCAL_RANK" in os.environ
    )
    
    if ddp_enabled:
        # åˆå§‹åŒ– DDP
        # è®¾ç½® NCCL ç¯å¢ƒå˜é‡ä»¥è°ƒè¯•å’Œé˜²æ­¢æ­»é”
        os.environ.setdefault("NCCL_TIMEOUT", "1800")  # 30åˆ†é’Ÿ
        os.environ["NCCL_P2P_DISABLE"] = "1"  # ç¦ç”¨ P2P é˜²æ­¢ 2080Ti å¯èƒ½å‡ºç°çš„ P2P æ­»é”
        os.environ["NCCL_BLOCKING_WAIT"] = "1"  # é˜»å¡ç­‰å¾…ï¼ŒæŠ¥é”™æ—¶æä¾›æ›´å¤šä¿¡æ¯
        # å¯é€‰ï¼šå¦‚æœéœ€è¦è¯¦ç»†è°ƒè¯•æ—¥å¿—ï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Š
        # os.environ.setdefault("NCCL_DEBUG", "INFO")
        
        # è·å–ç¯å¢ƒå˜é‡
        if "LOCAL_RANK" in os.environ:
            local_rank = int(os.environ["LOCAL_RANK"])
            rank = int(os.environ.get("RANK", local_rank))
            world_size = int(os.environ.get("WORLD_SIZE", 1))
        else:
            rank = int(os.environ.get("RANK", 0))
            world_size = int(os.environ.get("WORLD_SIZE", 1))
            local_rank = rank % torch.cuda.device_count()
        
        # è®¾ç½®å½“å‰è®¾å¤‡ï¼ˆå¿…é¡»åœ¨ init_process_group ä¹‹å‰ï¼‰
        torch.cuda.set_device(local_rank)
        device = torch.device(f"cuda:{local_rank}")
        
        # åˆå§‹åŒ–è¿›ç¨‹ç»„ï¼Œè®¾ç½®è¶…æ—¶æ—¶é—´ä¸º 30 åˆ†é’Ÿ
        # å¦‚æœä½¿ç”¨ torchrunï¼Œç¯å¢ƒå˜é‡ä¼šè‡ªåŠ¨è®¾ç½®ï¼Œä¸éœ€è¦æ‰‹åŠ¨æŒ‡å®š init_method
        # å¦‚æœæ‰‹åŠ¨å¯åŠ¨ï¼Œå¯ä»¥ä½¿ç”¨ init_method
        if "MASTER_ADDR" in os.environ and "MASTER_PORT" in os.environ:
            # torchrun æ¨¡å¼ï¼šä½¿ç”¨ç¯å¢ƒå˜é‡è‡ªåŠ¨åˆå§‹åŒ–
            dist.init_process_group(
                backend="nccl",
                timeout=timedelta(minutes=30)  # 30åˆ†é’Ÿè¶…æ—¶
            )
        else:
            # æ‰‹åŠ¨æ¨¡å¼ï¼šä½¿ç”¨æŒ‡å®šçš„ master_port
            dist.init_process_group(
                backend="nccl",
                init_method=f"tcp://127.0.0.1:{args.master_port}",
                rank=rank,
                world_size=world_size,
                timeout=timedelta(minutes=30)
            )
        
        # åªåœ¨ rank 0 æ‰“å°ä¿¡æ¯
        if rank == 0:
            print("="*70)
            print("è›‹ç™½è´¨ä¾§é“¾è®¾è®¡æ¨¡å‹ - è®­ç»ƒ (DDP æ¨¡å¼)")
            print("="*70)
            print(f"DDP æ¨¡å¼: å¯ç”¨ (world_size={world_size})")
            print(f"è®¾å¤‡: {device} (rank={rank}, local_rank={local_rank})")
            print(f"NCCL_TIMEOUT: {os.environ.get('NCCL_TIMEOUT', 'default')}")
    else:
        # å•GPUæ¨¡å¼
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        rank = 0
        world_size = 1
        local_rank = 0
        
        print("="*70)
        print("è›‹ç™½è´¨ä¾§é“¾è®¾è®¡æ¨¡å‹ - è®­ç»ƒ")
        print("="*70)
        print(f"è®¾å¤‡: {device} (å•GPUæ¨¡å¼)")
    
    # åªåœ¨ rank 0 æ‰“å°ä¿¡æ¯
    if rank == 0:
        print(f"PDB è·¯å¾„: {args.pdb_path}")
        print(f"æ‰¹æ¬¡å¤§å°: {args.batch_size} (æ¯ä¸ªGPU)")
        if ddp_enabled:
            print(f"æ€»æ‰¹æ¬¡å¤§å°: {args.batch_size * world_size} (æ‰€æœ‰GPU)")
        print(f"è®­ç»ƒè½®æ•°: {args.epochs}")
        print(f"æ‰©æ•£æ¨¡å‹: å¯ç”¨ (æ—¶é—´æ­¥æ•°: {args.num_diffusion_steps})")
        print(f"Label smoothing: {args.label_smoothing}")
        print(f"æ©ç æ¯”ä¾‹: åŠ¨æ€ (Cosine Schedule, t=0æ—¶0%, t=1æ—¶100%)")
        print(f"æ©ç ç­–ç•¥: {args.masking_strategy}")
        print("="*70)

    if not (0.0 <= args.label_smoothing <= 1.0):
        raise ValueError("--label_smoothing å¿…é¡»åœ¨ [0, 1] èŒƒå›´å†…")
    
    # åŠ è½½æ•°æ®é›†
    if rank == 0:
        print("\n1. åŠ è½½æ•°æ®é›†...")
    
    # å°è¯•ä½¿ç”¨é¢„å®šä¹‰çš„åˆ’åˆ†
    # å¦‚æœæŒ‡å®šäº† --use_predefined_splitï¼Œæˆ–è€… cache_dir å­˜åœ¨ä¸”åŒ…å« train.txt å’Œ val.txtï¼Œåˆ™ä½¿ç”¨é¢„å®šä¹‰åˆ’åˆ†
    use_predefined = args.use_predefined_split
    train_paths = None
    val_paths = None
    
    if args.cache_dir:
        train_paths, val_paths = load_train_val_splits(args.cache_dir)
        if train_paths is not None and val_paths is not None and len(train_paths) > 0 and len(val_paths) > 0:
            # å¦‚æœæ‰¾åˆ°äº†é¢„å®šä¹‰åˆ’åˆ†ï¼Œè‡ªåŠ¨ä½¿ç”¨ï¼ˆé™¤éæ˜ç¡®æŒ‡å®šä¸ä½¿ç”¨ï¼‰
            if not args.use_predefined_split:
                # è‡ªåŠ¨æ£€æµ‹ï¼šå¦‚æœå­˜åœ¨é¢„å®šä¹‰æ–‡ä»¶ï¼Œé»˜è®¤ä½¿ç”¨
                use_predefined = True
            if rank == 0:
                print(f"   âœ… ä½¿ç”¨é¢„å®šä¹‰çš„æ•°æ®é›†åˆ’åˆ†")
                print(f"   è®­ç»ƒé›†æ–‡ä»¶: {len(train_paths)} ä¸ª")
                print(f"   éªŒè¯é›†æ–‡ä»¶: {len(val_paths)} ä¸ª")
        else:
            if args.use_predefined_split:
                if rank == 0:
                    print(f"   âš ï¸  é¢„å®šä¹‰åˆ’åˆ†æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨éšæœºåˆ’åˆ†")
                use_predefined = False
            else:
                use_predefined = False
    
    # æ ¹æ®æ˜¯å¦ä½¿ç”¨é¢„å®šä¹‰åˆ’åˆ†æ¥åŠ è½½æ•°æ®é›†
    if use_predefined and train_paths is not None and val_paths is not None:
        # ä½¿ç”¨é¢„å®šä¹‰çš„åˆ’åˆ†
        # è®­ç»ƒé›†å¯ç”¨æ•°æ®å¢å¼ºï¼ŒéªŒè¯é›†ç¦ç”¨æ•°æ®å¢å¼ºä»¥ç¡®ä¿ç¨³å®šçš„éªŒè¯æŒ‡æ ‡
        train_dataset = ProteinStructureDataset(
            train_paths,
            cache_dir=args.cache_dir,
            augment=True  # è®­ç»ƒæ—¶å¯ç”¨æ•°æ®å¢å¼º
        )
        val_dataset = ProteinStructureDataset(
            val_paths,
            cache_dir=args.cache_dir,
            augment=False  # éªŒè¯æ—¶ç¦ç”¨æ•°æ®å¢å¼ºï¼Œç¡®ä¿ç¨³å®šçš„éªŒè¯æŸå¤±
        )
        if rank == 0:
            print(f"   è®­ç»ƒé›†: {len(train_dataset)} ä¸ªæ ·æœ¬")
            print(f"   éªŒè¯é›†: {len(val_dataset)} ä¸ªæ ·æœ¬")
    else:
        # ä½¿ç”¨éšæœºåˆ’åˆ†æˆ–å…¨éƒ¨æ•°æ®
        # å…ˆåˆ›å»ºä¸€ä¸ªä¸´æ—¶æ•°æ®é›†ä»¥è·å–å¤§å°å’Œæ–‡ä»¶åˆ—è¡¨
        temp_dataset = ProteinStructureDataset(
            args.pdb_path,
            cache_dir=args.cache_dir,
            augment=False  # ä¸´æ—¶æ•°æ®é›†ï¼Œaugment å‚æ•°ä¸é‡è¦
        )
        if rank == 0:
            print(f"   æ•°æ®é›†å¤§å°: {len(temp_dataset)}")
            if args.cache_dir:
                print(f"   ä½¿ç”¨ç¼“å­˜ç›®å½•: {args.cache_dir}")
        
        # åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
        if args.val_split > 0:
            val_size = int(len(temp_dataset) * args.val_split)
            train_size = len(temp_dataset) - val_size
            train_indices, val_indices = torch.utils.data.random_split(
                range(len(temp_dataset)), [train_size, val_size]
            )
            
            # åˆ›å»ºè®­ç»ƒé›†ï¼ˆå¯ç”¨æ•°æ®å¢å¼ºï¼‰
            train_dataset = torch.utils.data.Subset(
                ProteinStructureDataset(
                    args.pdb_path,
                    cache_dir=args.cache_dir,
                    augment=True  # è®­ç»ƒæ—¶å¯ç”¨æ•°æ®å¢å¼º
                ),
                train_indices.indices
            )
            
            # åˆ›å»ºéªŒè¯é›†ï¼ˆç¦ç”¨æ•°æ®å¢å¼ºï¼‰
            val_dataset = torch.utils.data.Subset(
                ProteinStructureDataset(
                    args.pdb_path,
                    cache_dir=args.cache_dir,
                    augment=False  # éªŒè¯æ—¶ç¦ç”¨æ•°æ®å¢å¼ºï¼Œç¡®ä¿ç¨³å®šçš„éªŒè¯æŸå¤±
                ),
                val_indices.indices
            )
            
            if rank == 0:
                print(f"   è®­ç»ƒé›†: {len(train_dataset)}, éªŒè¯é›†: {len(val_dataset)} (éšæœºåˆ’åˆ†)")
        else:
            # å…¨éƒ¨æ•°æ®ä½œä¸ºè®­ç»ƒé›†ï¼Œå¯ç”¨æ•°æ®å¢å¼º
            train_dataset = ProteinStructureDataset(
                args.pdb_path,
                cache_dir=args.cache_dir,
                augment=True  # è®­ç»ƒæ—¶å¯ç”¨æ•°æ®å¢å¼º
            )
            val_dataset = None

    # è¯Šæ–­é€‰é¡¹ï¼šé™åˆ¶æ ·æœ¬æ•°é‡ï¼ˆå¿«é€Ÿå®éªŒï¼‰
    if args.max_train_samples > 0 and len(train_dataset) > args.max_train_samples:
        train_dataset = torch.utils.data.Subset(train_dataset, list(range(args.max_train_samples)))
        if rank == 0:
            print(f"   ğŸ”¬ è®­ç»ƒé›†å·²æˆªæ–­ä¸º: {len(train_dataset)} ä¸ªæ ·æœ¬")

    if val_dataset is not None and args.max_val_samples > 0 and len(val_dataset) > args.max_val_samples:
        val_dataset = torch.utils.data.Subset(val_dataset, list(range(args.max_val_samples)))
        if rank == 0:
            print(f"   ğŸ”¬ éªŒè¯é›†å·²æˆªæ–­ä¸º: {len(val_dataset)} ä¸ªæ ·æœ¬")

    # å®¹é‡è¯Šæ–­ï¼šè®©æ¨¡å‹åœ¨å°è®­ç»ƒå­é›†ä¸Šè¿‡æ‹Ÿåˆï¼ŒéªŒè¯æ˜¯å¦å…·å¤‡è¡¨è¾¾èƒ½åŠ›
    if args.overfit_train_subset > 0:
        overfit_n = min(args.overfit_train_subset, len(train_dataset))
        overfit_indices = list(range(overfit_n))
        train_dataset = torch.utils.data.Subset(train_dataset, overfit_indices)
        val_dataset = torch.utils.data.Subset(train_dataset, list(range(overfit_n)))
        if rank == 0:
            print(f"   ğŸ§ª è¿‡æ‹Ÿåˆè¯Šæ–­æ¨¡å¼: train=val={overfit_n} ä¸ªæ ·æœ¬")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    # DDP æ¨¡å¼ï¼šä½¿ç”¨ DistributedSampler
    if ddp_enabled:
        train_sampler = DistributedSampler(
            train_dataset,
            num_replicas=world_size,
            rank=rank,
            shuffle=True,
            drop_last=True  # é˜²æ­¢æœ€åä¸€ä¸ª batch å¤§å°ä¸ä¸€è‡´å¯¼è‡´çš„ DDP åŒæ­¥æŒ‚èµ·
        )
        train_loader = DataLoader(
            train_dataset,
            batch_size=args.batch_size,
            sampler=train_sampler,  # ä½¿ç”¨ sampler æ—¶ä¸èƒ½è®¾ç½® shuffle
            collate_fn=collate_fn,
            num_workers=4,
            pin_memory=True,
            persistent_workers=False,  # æœ€å®‰å…¨çš„è®¾ç½®ï¼Œé˜²æ­¢ epoch åˆ‡æ¢æ­»é”
            drop_last=True  # å¼ºåˆ¶è®¾ç½®ï¼Œé˜²æ­¢æœ€åä¸€ä¸ª batch å¤§å°ä¸ä¸€è‡´
        )
        
        val_sampler = None
        if val_dataset is not None:
            val_sampler = DistributedSampler(
                val_dataset,
                num_replicas=world_size,
                rank=rank,
                shuffle=False,  # éªŒè¯é›†ä¸éœ€è¦ shuffle
                drop_last=True
            )
            val_batch_size = max(1, args.batch_size // 2)  # éªŒè¯é›†å‡åŠä»¥é™ä½æ˜¾å­˜å‹åŠ›
            val_loader = DataLoader(
                val_dataset,
                batch_size=val_batch_size,
                sampler=val_sampler,
                collate_fn=collate_fn,
                num_workers=4,
                pin_memory=True,
                persistent_workers=False,
                drop_last=True
            )
        else:
            val_loader = None
    else:
        # å•GPUæ¨¡å¼ï¼šä½¿ç”¨æ™®é€š DataLoader
        train_sampler = None
        train_loader = DataLoader(
            train_dataset,
            batch_size=args.batch_size,
            shuffle=True,
            collate_fn=collate_fn,
            num_workers=4,
            pin_memory=True,
            persistent_workers=False,
            drop_last=True
        )
        
        val_loader = None
        val_sampler = None
        if val_dataset is not None:
            val_batch_size = max(1, args.batch_size // 2)  # éªŒè¯é›†å‡åŠä»¥é™ä½æ˜¾å­˜å‹åŠ›
            val_loader = DataLoader(
                val_dataset,
                batch_size=val_batch_size,
                shuffle=False,
                collate_fn=collate_fn,
                num_workers=4,
                pin_memory=True,
                persistent_workers=False,
                drop_last=True
            )
    
    # åˆå§‹åŒ–æ¨¡å‹
    if rank == 0:
        print("\n2. åˆå§‹åŒ–æ¨¡å‹...")
    vocab = get_vocab()
    encoder = BackboneEncoder(
        hidden_dim=args.hidden_dim,
        num_layers=args.num_encoder_layers,
        k_neighbors=30,
        dropout=args.dropout  # ä¼ é€’ dropout å‚æ•°ä»¥å¢å¼ºæ­£åˆ™åŒ–
    )
    decoder = FragmentDecoder(
        input_dim=args.hidden_dim,
        vocab_size=vocab.get_vocab_size(),
        num_torsion_bins=72,
        hidden_dim=args.hidden_dim,
        num_layers=args.num_decoder_layers,
        num_heads=args.num_heads,
        dropout=args.dropout  # ä¼ é€’ dropout å‚æ•°ä»¥å¢å¼ºæ­£åˆ™åŒ–
    )
    
    if rank == 0:
        encoder_params = sum(p.numel() for p in encoder.parameters())
        decoder_params = sum(p.numel() for p in decoder.parameters())
        print(f"   Encoder å‚æ•°: {encoder_params:,}")
        print(f"   Decoder å‚æ•°: {decoder_params:,}")
        print(f"   æ€»å‚æ•°: {encoder_params + decoder_params:,}")
    
    # ç§»åŠ¨åˆ°è®¾å¤‡
    encoder.to(device)
    decoder.to(device)
    
    # DDP æ¨¡å¼ï¼šåœ¨æ¨¡å‹åˆå§‹åŒ–åã€DDP åŒ…è£…å‰æ·»åŠ  barrier
    # ç¡®ä¿ Rank 0 åŠ è½½å®Œé…ç½®/è¯è¡¨åï¼Œæ‰€æœ‰è¿›ç¨‹å†ä¸€èµ·åŒ…è£… DDP
    if ddp_enabled:
        dist.barrier()
        if rank == 0:
            print("   âœ… æ‰€æœ‰è¿›ç¨‹å·²å®Œæˆæ¨¡å‹åˆå§‹åŒ–ï¼Œå¼€å§‹ DDP åŒ…è£…...")
    
    # DDP æ¨¡å¼ï¼šåŒ…è£…æ¨¡å‹
    if ddp_enabled:
        encoder = torch.nn.parallel.DistributedDataParallel(
            encoder,
            device_ids=[local_rank],
            output_device=local_rank,
            find_unused_parameters=False,  # å¦‚æœæ‰€æœ‰å‚æ•°éƒ½è¢«ä½¿ç”¨ï¼Œè®¾ä¸º False å¯ä»¥æå‡æ€§èƒ½
            broadcast_buffers=False  # æ¨¡å‹æœªä½¿ç”¨BatchNormï¼Œå…³é—­å‰å‘bufferåŒæ­¥å¯é™ä½æ­»é”é£é™©
        )
        decoder = torch.nn.parallel.DistributedDataParallel(
            decoder,
            device_ids=[local_rank],
            output_device=local_rank,
            find_unused_parameters=False,
            broadcast_buffers=False
        )
        if rank == 0:
            print(f"   âœ… æ¨¡å‹å·²åŒ…è£…ä¸º DDP (device_id={local_rank})")
    
    # åˆå§‹åŒ–è®­ç»ƒå™¨
    if rank == 0:
        print("\n3. åˆå§‹åŒ–è®­ç»ƒå™¨...")
    trainer = Trainer(
        encoder=encoder,
        decoder=decoder,
        train_loader=train_loader,
        val_loader=val_loader,
        device=device,
        learning_rate=args.learning_rate,
        weight_decay=args.weight_decay,
        masking_strategy=args.masking_strategy,
        num_diffusion_steps=args.num_diffusion_steps,
        warmup_epochs=args.warmup_epochs,
        total_epochs=args.epochs,
        ddp_enabled=ddp_enabled,
        rank=rank,
        world_size=world_size,
        train_sampler=train_sampler,
        val_sampler=val_sampler,
        debug_mode=args.debug_mode,
        label_smoothing=args.label_smoothing
    )
    
    # å¼€å§‹è®­ç»ƒ
    if rank == 0:
        print("\n4. å¼€å§‹è®­ç»ƒ...")
        if args.visualize:
            print(f"   å¯è§†åŒ–: å¯ç”¨ (æ¯ {args.plot_every} ä¸ª epoch ç»˜åˆ¶ä¸€æ¬¡)")
    trainer.train(
        num_epochs=args.epochs,
        save_dir=args.save_dir,
        save_every=10,
        visualize=args.visualize,
        plot_every=args.plot_every,
        resume_from=args.resume,
        early_stopping_patience=args.early_stopping_patience,
        early_stopping_min_delta=args.early_stopping_min_delta
    )
    
    # æ¸…ç† DDP
    if ddp_enabled:
        dist.destroy_process_group()
    
    if rank == 0:
        print("\n" + "="*70)
        print("è®­ç»ƒå®Œæˆï¼")
        print(f"æ¨¡å‹ä¿å­˜åœ¨: {args.save_dir}")
        print("="*70)


if __name__ == "__main__":
    # å…³é”®ï¼šåœ¨ if __name__ == "__main__": çš„æœ€å¼€å§‹è§£æå‚æ•°
    # è¿™æ ·å¯ä»¥åœ¨æ‰€æœ‰ torch å¼•ç”¨ä¹‹å‰è®¾ç½® CUDA_VISIBLE_DEVICES
    _args = parse_args()
    
    # å°† args å­˜å‚¨åˆ°å…¨å±€å‘½åç©ºé—´ï¼Œä¾› main() ä½¿ç”¨
    # main() å‡½æ•°å†…éƒ¨ä¼šåœ¨æœ€å¼€å§‹è®¾ç½® CUDA_VISIBLE_DEVICES
    globals()['_args'] = _args
    
    # è°ƒç”¨ä¸»å‡½æ•°ï¼ˆmain() å†…éƒ¨ä¼šè®¾ç½® CUDA_VISIBLE_DEVICES å¹¶å¯¼å…¥ torch ç­‰æ¨¡å—ï¼‰
    main()
