"""
ç»¼åˆæµ‹è¯•è„šæœ¬ - æµ‹è¯•æ‰€æœ‰æ ¸å¿ƒæ¨¡å—

è¿è¡Œæ­¤è„šæœ¬å¯ä»¥æµ‹è¯•é¡¹ç›®çš„æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½ï¼š
1. è¯æ±‡è¡¨æ¨¡å—
2. å‡ ä½•å·¥å…·æ¨¡å—
3. æ¨¡å‹å‰å‘ä¼ æ’­
4. æ•°æ®é›†åŠ è½½ï¼ˆå¦‚æœæä¾› PDB æ–‡ä»¶ï¼‰

ä½¿ç”¨æ–¹æ³•:
    python test_all.py
    python test_all.py --pdb_path path/to/protein.pdb
"""

import sys
import torch
from typing import Optional

print("="*70)
print("Protein MDM - ç»¼åˆæµ‹è¯•è„šæœ¬")
print("="*70)

# ============================================================================
# æµ‹è¯• 1: è¯æ±‡è¡¨æ¨¡å— (Vocabulary)
# ============================================================================
print("\n" + "="*70)
print("æµ‹è¯• 1: è¯æ±‡è¡¨æ¨¡å— (FragmentVocab)")
print("="*70)

try:
    from data.vocabulary import FragmentVocab, get_vocab, SpecialTokens
    
    vocab = get_vocab()
    print(f"âœ… è¯æ±‡è¡¨åˆå§‹åŒ–æˆåŠŸ")
    print(f"   - è¯æ±‡è¡¨å¤§å°: {vocab.get_vocab_size()}")
    print(f"   - ç‰‡æ®µæ•°é‡: {vocab.get_fragment_count()}")
    print(f"   - ç‰¹æ®Š Token: {[vocab.idx_to_token[i] for i in range(4)]}")
    
    # æµ‹è¯•æ‰€æœ‰ 20 ç§æ°¨åŸºé…¸
    print("\n   æµ‹è¯• 20 ç§æ ‡å‡†æ°¨åŸºé…¸æ˜ å°„:")
    test_residues = [
        "ALA", "VAL", "LEU", "ILE", "MET",  # éææ€§è„‚è‚ªæ—
        "PHE", "TYR", "TRP",                  # èŠ³é¦™æ—
        "SER", "THR", "ASN", "GLN",          # ææ€§ä¸å¸¦ç”µ
        "LYS", "ARG", "HIS",                 # æ­£ç”µ
        "ASP", "GLU",                        # è´Ÿç”µ
        "CYS", "GLY", "PRO"                  # ç‰¹æ®Š
    ]
    
    success_count = 0
    for res in test_residues:
        try:
            fragments = vocab.residue_to_fragments(res)
            indices = vocab.fragments_to_indices(fragments)
            print(f"   âœ“ {res:3s} -> {fragments} -> {indices}")
            success_count += 1
        except Exception as e:
            print(f"   âœ— {res:3s} -> é”™è¯¯: {e}")
    
    print(f"\n   âœ… æˆåŠŸæ˜ å°„ {success_count}/20 ç§æ°¨åŸºé…¸")
    
    # æµ‹è¯•é”™è¯¯å¤„ç†
    try:
        vocab.residue_to_fragments("XXX")
        print("   âœ— é”™è¯¯å¤„ç†æµ‹è¯•å¤±è´¥: åº”è¯¥æŠ›å‡º KeyError")
    except KeyError:
        print("   âœ… é”™è¯¯å¤„ç†æµ‹è¯•é€šè¿‡: æœªçŸ¥æ®‹åŸºæ­£ç¡®æŠ›å‡º KeyError")
    
    vocab_test_passed = True
    
except Exception as e:
    print(f"âŒ è¯æ±‡è¡¨æµ‹è¯•å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    vocab_test_passed = False

# ============================================================================
# æµ‹è¯• 2: å‡ ä½•å·¥å…·æ¨¡å— (Geometry)
# ============================================================================
print("\n" + "="*70)
print("æµ‹è¯• 2: å‡ ä½•å·¥å…·æ¨¡å— (Torsion Angles)")
print("="*70)

try:
    import numpy as np
    from data.geometry import (
        calculate_dihedrals,
        discretize_angle,
        discretize_angles,
        undiscretize_angle,
        undiscretize_angles,
        get_torsion_angle_resolution
    )
    
    print("âœ… å‡ ä½•æ¨¡å—å¯¼å…¥æˆåŠŸ")
    
    # æµ‹è¯•è§’åº¦ç¦»æ•£åŒ–
    print("\n   æµ‹è¯•è§’åº¦ç¦»æ•£åŒ– (72 bins = 5åº¦åˆ†è¾¨ç‡):")
    test_angles = [
        -np.pi, -np.pi/2, 0, np.pi/2, np.pi
    ]
    
    for angle in test_angles:
        bin_idx = discretize_angle(angle, num_bins=72)
        angle_recovered = undiscretize_angle(bin_idx, num_bins=72)
        error = abs(angle - angle_recovered)
        print(f"   âœ“ {np.degrees(angle):7.2f}Â° -> Bin {bin_idx:3d} -> {np.degrees(angle_recovered):7.2f}Â° (è¯¯å·®: {np.degrees(error):.2f}Â°)")
    
    # æµ‹è¯•å‘é‡åŒ–æ“ä½œ
    print("\n   æµ‹è¯•å‘é‡åŒ–æ“ä½œ:")
    angles_array = np.linspace(-np.pi, np.pi, 10)
    bin_indices = discretize_angles(angles_array, num_bins=72)
    angles_recovered = undiscretize_angles(bin_indices, num_bins=72)
    max_error = np.max(np.abs(angles_array - angles_recovered))
    print(f"   âœ“ å‘é‡åŒ–æµ‹è¯•é€šè¿‡ (æœ€å¤§è¯¯å·®: {np.degrees(max_error):.2f}Â°)")
    
    # æµ‹è¯•äºŒé¢è§’è®¡ç®—ï¼ˆéœ€è¦ BioPythonï¼‰
    print("\n   æµ‹è¯•äºŒé¢è§’è®¡ç®—:")
    try:
        coords = np.array([
            [0.0, 0.0, 0.0],
            [1.0, 0.0, 0.0],
            [1.0, 1.0, 0.0],
            [0.0, 1.0, 0.0],
        ])
        angle = calculate_dihedrals(coords, [(0, 1, 2, 3)])[0]
        print(f"   âœ“ äºŒé¢è§’è®¡ç®—æˆåŠŸ: {np.degrees(angle):.2f}Â°")
    except Exception as e:
        print(f"   âš  äºŒé¢è§’è®¡ç®—éœ€è¦ BioPython: {e}")
    
    print(f"\n   âœ… åˆ†è¾¨ç‡: {get_torsion_angle_resolution(72):.2f} åº¦/æ¯ bin")
    geometry_test_passed = True
    
except Exception as e:
    print(f"âŒ å‡ ä½•æ¨¡å—æµ‹è¯•å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    geometry_test_passed = False

# ============================================================================
# æµ‹è¯• 3: æ¨¡å‹å‰å‘ä¼ æ’­
# ============================================================================
print("\n" + "="*70)
print("æµ‹è¯• 3: æ¨¡å‹å‰å‘ä¼ æ’­ (Encoder + Decoder)")
print("="*70)

try:
    from models.encoder import BackboneEncoder
    from models.decoder import FragmentDecoder
    from data.vocabulary import get_vocab
    
    vocab = get_vocab()
    
    # åˆå§‹åŒ–æ¨¡å‹
    hidden_dim = 256
    encoder = BackboneEncoder(hidden_dim=hidden_dim)
    decoder = FragmentDecoder(
        input_dim=hidden_dim,
        vocab_size=vocab.get_vocab_size(),
        num_torsion_bins=72,
        hidden_dim=hidden_dim
    )
    
    print("âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
    print(f"   - Encoder è¾“å‡ºç»´åº¦: {encoder.get_output_dim()}")
    print(f"   - Decoder è¯æ±‡è¡¨å¤§å°: {decoder.vocab_size}")
    print(f"   - Decoder æ‰­è½¬è§’ bins: {decoder.num_torsion_bins}")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    print("\n   æµ‹è¯•å‰å‘ä¼ æ’­:")
    batch_size = 2
    seq_len = 10
    frag_seq_len = 20
    
    # åˆ›å»ºè™šæ‹Ÿéª¨æ¶åæ ‡ [batch_size, seq_len, 4 atoms, 3 coords]
    dummy_backbone = torch.randn(batch_size, seq_len, 4, 3)
    print(f"   - è¾“å…¥å½¢çŠ¶: {dummy_backbone.shape}")
    
    # Encoder å‰å‘ä¼ æ’­
    node_embeddings = encoder(dummy_backbone)
    print(f"   - Encoder è¾“å‡ºå½¢çŠ¶: {node_embeddings.shape}")
    assert node_embeddings.shape == (batch_size, seq_len, hidden_dim), \
        f"Encoder è¾“å‡ºå½¢çŠ¶é”™è¯¯: {node_embeddings.shape}"
    
    # åˆ›å»ºç›®æ ‡ç‰‡æ®µåºåˆ—ï¼ˆToken IDsï¼‰
    target_fragments = torch.randint(0, vocab.get_vocab_size(), (batch_size, frag_seq_len))
    
    # Decoder å‰å‘ä¼ æ’­
    frag_logits, tors_logits = decoder(
        node_embeddings=node_embeddings,
        target_fragments=target_fragments
    )
    print(f"   - Fragment logits å½¢çŠ¶: {frag_logits.shape}")
    print(f"   - Torsion logits å½¢çŠ¶: {tors_logits.shape}")
    
    # éªŒè¯è¾“å‡ºå½¢çŠ¶
    assert frag_logits.shape[0] == batch_size, "Fragment logits batch size é”™è¯¯"
    assert tors_logits.shape[0] == batch_size, "Torsion logits batch size é”™è¯¯"
    
    print("   âœ… å‰å‘ä¼ æ’­æµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯•æ¢¯åº¦
    print("\n   æµ‹è¯•æ¢¯åº¦è®¡ç®—:")
    loss = frag_logits.sum() + tors_logits.sum()
    loss.backward()
    print("   âœ… æ¢¯åº¦è®¡ç®—æˆåŠŸ")
    
    model_test_passed = True
    
except Exception as e:
    print(f"âŒ æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    model_test_passed = False

# ============================================================================
# æµ‹è¯• 4: æ•°æ®é›†åŠ è½½ (å¯é€‰ï¼Œéœ€è¦ PDB æ–‡ä»¶)
# ============================================================================
print("\n" + "="*70)
print("æµ‹è¯• 4: æ•°æ®é›†åŠ è½½ (éœ€è¦ PDB æ–‡ä»¶)")
print("="*70)

pdb_path = None
if len(sys.argv) > 1:
    import argparse
    import os
    parser = argparse.ArgumentParser()
    parser.add_argument("--pdb_path", type=str, default=None)
    args = parser.parse_args()
    pdb_path = args.pdb_path
    
    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ
    if pdb_path and not os.path.exists(pdb_path):
        print(f"   âš ï¸  PDB è·¯å¾„ä¸å­˜åœ¨: {pdb_path}")
        print("   âš ï¸  è·³è¿‡æ•°æ®é›†æµ‹è¯•")
        pdb_path = None

if pdb_path:
    try:
        from data.dataset import ProteinStructureDataset, collate_fn
        from torch.utils.data import DataLoader
        
        print(f"   åŠ è½½ PDB æ–‡ä»¶: {pdb_path}")
        dataset = ProteinStructureDataset(pdb_path)
        print(f"   âœ… æ•°æ®é›†åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(dataset)} ä¸ªæ ·æœ¬")
        
        if len(dataset) > 0:
            # æµ‹è¯•å•ä¸ªæ ·æœ¬
            sample = dataset[0]
            print(f"\n   æ ·æœ¬ä¿¡æ¯:")
            print(f"   - éª¨æ¶åæ ‡å½¢çŠ¶: {sample['backbone_coords'].shape}")
            print(f"   - ç‰‡æ®µ Token æ•°é‡: {len(sample['fragment_token_ids'])}")
            print(f"   - æ‰­è½¬è§’ bins æ•°é‡: {len(sample['torsion_bins'])}")
            print(f"   - åºåˆ—é•¿åº¦: {sample['sequence_length'].item()}")
            print(f"   - æ®‹åŸºç±»å‹: {sample['residue_types'][:5]}...")  # æ˜¾ç¤ºå‰5ä¸ª
            
            # æµ‹è¯• DataLoader
            dataloader = DataLoader(
                dataset,
                batch_size=2,
                collate_fn=collate_fn,
                shuffle=False
            )
            batch = next(iter(dataloader))
            print(f"\n   æ‰¹å¤„ç†ä¿¡æ¯:")
            print(f"   - æ‰¹å¤„ç†éª¨æ¶å½¢çŠ¶: {batch['backbone_coords'].shape}")
            print(f"   - æ‰¹å¤„ç†ç‰‡æ®µå½¢çŠ¶: {batch['fragment_token_ids'].shape}")
            print(f"   - æ‰¹å¤„ç†æ‰­è½¬è§’å½¢çŠ¶: {batch['torsion_bins'].shape}")
            print(f"   - åºåˆ—é•¿åº¦: {batch['sequence_lengths']}")
            
            print("   âœ… æ•°æ®é›†æµ‹è¯•é€šè¿‡")
            dataset_test_passed = True
        else:
            print("   âš  æ•°æ®é›†ä¸ºç©º")
            dataset_test_passed = None
            
    except Exception as e:
        print(f"   âŒ æ•°æ®é›†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        dataset_test_passed = False
else:
    print("   âš ï¸  è·³è¿‡æ•°æ®é›†æµ‹è¯• (æœªæä¾›æœ‰æ•ˆçš„ PDB æ–‡ä»¶)")
    print("   æç¤º: ä½¿ç”¨ --pdb_path path/to/protein.pdb æ¥æµ‹è¯•æ•°æ®é›†åŠ è½½")
    print("   æ³¨æ„: PDB æ–‡ä»¶å¿…é¡»å­˜åœ¨ä¸”å¯è¯»")
    dataset_test_passed = None

# ============================================================================
# æµ‹è¯•æ€»ç»“
# ============================================================================
print("\n" + "="*70)
print("æµ‹è¯•æ€»ç»“")
print("="*70)

tests = [
    ("è¯æ±‡è¡¨æ¨¡å—", vocab_test_passed),
    ("å‡ ä½•å·¥å…·æ¨¡å—", geometry_test_passed),
    ("æ¨¡å‹å‰å‘ä¼ æ’­", model_test_passed),
    ("æ•°æ®é›†åŠ è½½", dataset_test_passed),
]

passed = sum(1 for _, result in tests if result is True)
total = sum(1 for _, result in tests if result is not None)

for name, result in tests:
    if result is True:
        status = "âœ… é€šè¿‡"
    elif result is False:
        status = "âŒ å¤±è´¥"
    else:
        status = "âš ï¸  è·³è¿‡"
    print(f"   {name:20s}: {status}")

print(f"\n   æ€»è®¡: {passed}/{total} ä¸ªæµ‹è¯•é€šè¿‡")

if passed == total:
    print("\n   ğŸ‰ æ‰€æœ‰æ ¸å¿ƒæµ‹è¯•é€šè¿‡ï¼é¡¹ç›®åŸºç¡€åŠŸèƒ½æ­£å¸¸ã€‚")
else:
    print("\n   âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")

print("="*70)
